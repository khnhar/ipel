# -*- coding: utf-8 -*-
"""GoingDeeper(CV)_RS7) 3. 없다면 어떻게 될까? (ResNet Ablation Study) [프로젝트]

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V2QUfiLB_HXXs-74N6FYcMrfU7XrI1u3

###1) CIFAR-10
"""

!pip install tensorflow-datasets

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as cats_vs_dogs

import urllib3
urllib3.disable_warnings()

#tfds.disable_progress_bar()   # 이 주석을 풀면 데이터셋 다운로드과정의 프로그레스바가 나타나지 않습니다.

(ds_train, ds_test), ds_info = cats_vs_dogs.load(
    'cifar10',
    split=['train', 'test'],
    shuffle_files=True,
    with_info=True,
)

# Tensorflow 데이터셋을 로드하면 꼭 feature 정보를 확인해 보세요.
print(ds_info.features)

# 데이터의 개수도 확인해 봅시다.
print(tf.data.experimental.cardinality(ds_train))
print(tf.data.experimental.cardinality(ds_test))

"""###2) Input Normalization"""

# Q. 이미지의 표현이 0과 1 사이로 들어오도록 직접 정규화 코드를 작성해봅시다.

def normalize_and_resize_img(image, label):
    # 이미지 데이터를 float32 데이터 타입으로 변경합니다.
    image = tf.cast(image, tf.float32)
    # 이미지 데이터를 [0, 255] 범위에서 [0, 1] 범위로 정규화합니다.
    image /= 255.0
    return image, label

# 함수를 테스트하기 위해 임의의 이미지 데이터와 레이블을 생성하고 함수를 호출합니다.
import tensorflow as tf
import numpy as np

# 28x28 크기의 이미지를 생성하고 레이블을 1로 설정합니다.
test_image = np.random.randint(0, 256, size=(28, 28), dtype=np.uint8)
test_label = 1

# 함수 호출 및 결과 확인
normalize_and_resize_img(test_image, test_label)

def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):
    ds = ds.map(
        normalize_and_resize_img,
        num_parallel_calls=1
    )
    ds = ds.batch(batch_size)
    if not is_test:
        ds = ds.repeat()
        ds = ds.shuffle(200)
    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)
    return ds

ds_info.features["label"].num_classes

ds_info.features["label"].names

fig = cats_vs_dogs.show_examples(ds_train, ds_info)

fig = cats_vs_dogs.show_examples(ds_test, ds_info)

"""###VGG기본블록만들기"""

# function for building VGG Block

def build_vgg_block(input_layer,
                    num_cnn=3,
                    channel=64,
                    block_num=1,
                   ):
    # 입력 레이어
    x = input_layer

    # CNN 레이어
    for cnn_num in range(num_cnn):
        x = keras.layers.Conv2D(
            filters=channel,
            kernel_size=(3,3),
            activation='relu',
            kernel_initializer='he_normal',
            padding='same',
            name=f'block{block_num}_conv{cnn_num}'
        )(x)

    # Max Pooling 레이어
    x = keras.layers.MaxPooling2D(
        pool_size=(2, 2),
        strides=2,
        name=f'block{block_num}_pooling'
    )(x)

    return x

vgg_input_layer = keras.layers.Input(shape=(32,32,3))   # 입력 레이어 생성
vgg_block_output = build_vgg_block(vgg_input_layer)    # VGG 블록 생성

# 블록 1개짜리 model 생성
model = keras.Model(inputs=vgg_input_layer, outputs=vgg_block_output)

model.summary()

"""##(3) VGG Complete Model<BR>
####1) VGG-16
"""

# VGG 모델 자체를 생성하는 함수입니다.
def build_vgg(input_shape=(32,32,3),
              num_cnn_list=[2,2,3,3,3],
              channel_list=[64,128,256,512,512],
              num_classes=10):

    assert len(num_cnn_list) == len(channel_list) #모델을 만들기 전에 config list들이 같은 길이인지 확인합니다.

    input_layer = keras.layers.Input(shape=input_shape)  # input layer를 만들어둡니다.
    output = input_layer

    # config list들의 길이만큼 반복해서 블록을 생성합니다.
    for i, (num_cnn, channel) in enumerate(zip(num_cnn_list, channel_list)):
        output = build_vgg_block(
            output,
            num_cnn=num_cnn,
            channel=channel,
            block_num=i
        )

    output = keras.layers.Flatten(name='flatten')(output)
    output = keras.layers.Dense(4096, activation='relu', name='fc1')(output)
    output = keras.layers.Dense(4096, activation='relu', name='fc2')(output)
    output = keras.layers.Dense(num_classes, activation='softmax', name='predictions')(output)

    model = keras.Model(
        inputs=input_layer,
        outputs=output
    )
    return model

# 기본값을 그대로 사용해서 VGG 모델을 만들면 VGG-16이 됩니다.
vgg_16 = build_vgg()

vgg_16.summary()

"""####2) VGG-19"""

# 원하는 블록의 설계에 따라 매개변수로 리스트를 전달해 줍니다.
vgg_19 = build_vgg(
    num_cnn_list=[2,2,4,4,4],
    channel_list=[64,128,256,512,512]
)

vgg_19.summary()

## Q. VGG-16보다 작은 네트워크인 VGG-13을 만들어 보세요.

from tensorflow.keras import models, layers

def build_vgg(num_cnn_list, channel_list):
    model = models.Sequential()
    # 입력층
    model.add(layers.InputLayer(input_shape=(224, 224, 3)))

    # CNN 블록
    for num_cnn, channels in zip(num_cnn_list, channel_list):
        for _ in range(num_cnn):
            model.add(layers.Conv2D(channels, (3, 3), padding='same', activation='relu'))
        model.add(layers.MaxPooling2D((2, 2), strides=2))

    # 분류를 위한 완전 연결 레이어
    model.add(layers.Flatten())
    model.add(layers.Dense(4096, activation='relu'))
    model.add(layers.Dense(4096, activation='relu'))
    model.add(layers.Dense(1000, activation='softmax'))  # 1000개의 클래스를 가정

    return model

# VGG-13 모델 생성
vgg_13 = build_vgg(
    num_cnn_list=[2, 2, 2, 2],  # 각 블록당 2개의 CNN 레이어
    channel_list=[64, 128, 256, 512]  # 채널 수 설정
)

# 모델 구조 요약
vgg_13.summary()

"""## (4) VGG-16 vs VGG-19"""

BATCH_SIZE = 256
EPOCH = 15

(ds_train, ds_test), ds_info = cats_vs_dogs.load(
    'cifar10',
    split=['train', 'test'],
    as_supervised=True,
    shuffle_files=True,
    with_info=True,
)
ds_train = apply_normalize_on_dataset(ds_train, batch_size=BATCH_SIZE)
ds_test = apply_normalize_on_dataset(ds_test, batch_size=BATCH_SIZE)

vgg_16.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),
    metrics=['accuracy'],
)

history_16 = vgg_16.fit(
    ds_train,
    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),
    epochs=EPOCH,
    validation_data=ds_test,
    verbose=1,
    use_multiprocessing=True,
)

vgg_19.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.SGD(lr=0.01, clipnorm=1.),
    metrics=['accuracy'],
)

history_19 = vgg_19.fit(
    ds_train,
    steps_per_epoch=int(ds_info.splits['train'].num_examples/BATCH_SIZE),
    validation_steps=int(ds_info.splits['test'].num_examples/BATCH_SIZE),
    epochs=EPOCH,
    validation_data=ds_test,
    verbose=1,
    use_multiprocessing=True,
)

#훈련 손실(training loss)이 어떻게 다르게 진행되는지 비교

import matplotlib.pyplot as plt

plt.plot(history_16.history['loss'], 'r')
plt.plot(history_19.history['loss'], 'b')
plt.title('Model training loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['vgg_16', 'vgg_19'], loc='upper left')
plt.show()

#검증 정확도(validation accuracy)

plt.plot(history_16.history['val_accuracy'], 'r')
plt.plot(history_19.history['val_accuracy'], 'b')
plt.title('Model validation accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['vgg_16', 'vgg_19'], loc='upper left')
plt.show()

"""###ResNet Ablation Study

####0) 라이브러리 버전 확인하기
"""

import tensorflow as tf
import numpy as np

print(tf.__version__)
print(np.__version__)

"""####2) ResNet-34, ResNet-50 Complete Mode"""

resnet_34 = build_resnet(input_shape=(32, 32,3), is_50=False)
resnet_34.summary()

resnet_50 = build_resnet(input_shape=(32, 32,3), is_50=True)
resnet_50.summary()