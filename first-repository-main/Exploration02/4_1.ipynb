# First Test
!pip install xgboost==1.4.2
!pip install lightgbm==3.3.0
!pip install missingno==0.5.0
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
import warnings
warnings.filterwarnings("ignore")

import os
from os.path import join

import pandas as pd
import numpy as np

import missingno as msno

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold, cross_val_score
import xgboost as xgb
import lightgbm as lgb

import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

train_data_path = join(data_dir, 'train.csv')
sub_data_path = join(data_dir, 'test.csv')

print(train_data_path)
print(sub_data_path)
data = pd.read_csv(train_data_path)
sub = pd.read_csv(sub_data_path)
print(f'train data dim : {data.shape}')
print(f'sub data dim : {sub.shape}')
y = data['price']
del data['price']

print(data.columns)
train_len = len(data)
data = pd.concat((data, sub), axis=0)

print(len(data))
data.head()
!pip install matplotlib==3.4.3
import matplotlib

print(matplotlib.__version__)

msno.matrix(data) # 오류 걸리면 런타임 재실행하시면 됩니다.
# 1. id 컬럼이 결측치인지 확인합니다.
null_check = pd.isnull(data['id'])
print(null_check)
# 2. 결측치인 데이터만 뽑아냅니다.
null_data = data.loc[null_check, 'id']
null_data.head()
# 3. 결측치인 데이터의 개수를 셉니다.
print(f'id: {len(null_data.values)}')
print('{} : {}'.format('id', len(data.loc[pd.isnull(data['id']), 'id'].values)))
for c in data.columns:
    print('{} : {}'.format(c, len(data.loc[pd.isnull(data[c]), c].values)))
sub_id = data['id'][train_len:]
del data['id']

print(data.columns)
data['date'] = data['date'].apply(lambda x : str(x[:6]))

data.head()
fig, ax = plt.subplots(9, 2, figsize=(12, 50))   # 가로스크롤 때문에 그래프 확인이 불편하다면 figsize의 x값을 조절해 보세요.

# id 변수(count==0인 경우)는 제외하고 분포를 확인합니다.
count = 1
columns = data.columns
for row in range(9):
    for col in range(2):
        sns.kdeplot(data=data[columns[count]], ax=ax[row][col])
        ax[row][col].set_title(columns[count], fontsize=15)
        count += 1
        if count == 19 :
            break
skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']

for c in skew_columns:
    data[c] = np.log1p(data[c].values)

plt.figure(figsize=(15, 10))

for i, c in enumerate(skew_columns, 1):
    plt.subplot(3, 3, i)
    sns.kdeplot(data=data[c])

plt.tight_layout()
plt.show()
xx = np.linspace(0, 10, 500)
yy = np.log(xx)

plt.hlines(0, 0, 10)
plt.vlines(0, -5, 5)
plt.plot(xx, yy, c='r')
plt.show()
sns.kdeplot(y)
plt.show()
y_log_transformation = np.log1p(y)

sns.kdeplot(y_log_transformation)
plt.show()
sub = data.iloc[train_len:, :]
x = data.iloc[:train_len, :]

print(x.shape)
print(sub.shape)
gboost = GradientBoostingRegressor(random_state=2023)
xgboost = xgb.XGBRegressor(random_state=2023)
lightgbm = lgb.LGBMRegressor(random_state=2023)

models = [{'model':gboost, 'name':'GradientBoosting'}, {'model':xgboost, 'name':'XGBoost'},
          {'model':lightgbm, 'name':'LightGBM'}]

print('얍💢')
def get_cv_score(models):
    kfold = KFold(n_splits=5).get_n_splits(x.values)
    for m in models:
        CV_score = np.mean(cross_val_score(m['model'], X=x.values, y=y, cv=kfold))
        print(f"Model: {m['name']}, CV score:{CV_score:.4f}")
print('얍💢')
get_cv_score(models)
def AveragingBlending(models, x, y, sub_x):
    for m in models :
        m['model'].fit(x.values, y)

    predictions = np.column_stack([
        m['model'].predict(sub_x.values) for m in models
    ])
    return np.mean(predictions, axis=1)

print('얍💢')
y_pred = AveragingBlending(models, x, y, sub)
print(len(y_pred))
y_pred
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

submission_path = join(data_dir, 'sample_submission.csv')
submission = pd.read_csv(submission_path)
submission.head()
result = pd.DataFrame({
    'id' : sub_id,
    'price' : y_pred
})

result.head()
my_submission_path = join(data_dir, 'submission.csv')
result.to_csv(my_submission_path, index=False)

print(my_submission_path)

# Second Test(Ranking Up)
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

train_data_path = join(data_dir, 'train.csv')
test_data_path = join(data_dir, 'test.csv')

train = pd.read_csv(train_data_path)
test = pd.read_csv(test_data_path)

train.head()
train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)
train.head()
y = train['price']
del train['price']

print(train.columns)
del train['id']

print(train.columns)
test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)

del test['id']
print(test.columns)
test.head()
y
sns.kdeplot(y)
y = np.log1p(y)
y
sns.kdeplot(y)
plt.show()
train.info()
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

print('얍💢')
def rmse(y_test, y_pred):
    return np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))

print('얍💢')
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

print('얍💢')
random_state=2020

gboost = GradientBoostingRegressor(random_state=random_state)
xgboost = XGBRegressor(random_state=random_state)
lightgbm = LGBMRegressor(random_state=random_state)
rdforest = RandomForestRegressor(random_state=random_state)

models = [gboost, xgboost, lightgbm, rdforest]

print('얍💢')
gboost.__class__.__name__
df = {}

for model in models:
    # 모델 이름 획득
    model_name = model.__class__.__name__

    # train, test 데이터셋 분리
    # random_state를 사용하여 고정하고 train과 test 셋의 비율은 8:2로 합니다.
    X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=42)

    # 모델 학습
    model.fit(X_train, y_train)

    # 예측
    y_pred = model.predict(X_test)

    # 예측 결과의 rmse값 저장
    df[model_name] = rmse(y_test, y_pred)

    # data frame에 저장
    score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)

df
def get_scores(models, train, y):
    df = {}

    for model in models:
        # 모델 이름 획득
        model_name = model.__class__.__name__

        # train, test 데이터셋 분리
        # random_state를 사용하여 고정하고 train과 test 셋의 비율은 8:2로 합니다.
        X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=42)

        # 모델 학습
        model.fit(X_train, y_train)

        # 예측
        y_pred = model.predict(X_test)

        # 예측 결과의 rmse값 저장
        df[model_name] = rmse(y_test, y_pred)

        # data frame에 저장
        score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)


get_scores(models, train, y)
from sklearn.model_selection import GridSearchCV

print('얍💢')
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [1, 10],
}
model = LGBMRegressor(random_state=random_state)

print('얍💢')
grid_model = GridSearchCV(model, param_grid=param_grid, \
                        scoring='neg_mean_squared_error', \
                        cv=5, verbose=1, n_jobs=5)

grid_model.fit(train, y)
grid_model.cv_results_
params = grid_model.cv_results_['params']
params
score = grid_model.cv_results_['mean_test_score']
score
results = pd.DataFrame(params)
results['score'] = score

results
results['RMSE'] = np.sqrt(-1 * results['score'])
results
results = results.rename(columns={'RMSE': 'RMSLE'})
results
results = results.sort_values('RMSLE')
results
def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):
    # GridSearchCV 모델로 초기화
    grid_model = GridSearchCV(model, param_grid=param_grid, scoring='neg_mean_squared_error', \
                              cv=5, verbose=verbose, n_jobs=n_jobs)

    # 모델 fitting
    grid_model.fit(train, y)

    # 결과값 저장
    params = grid_model.cv_results_['params']
    score = grid_model.cv_results_['mean_test_score']

    # 데이터 프레임 생성
    results = pd.DataFrame(params)
    results['score'] = score

    # RMSLE 값 계산 후 정렬
    results['RMSLE'] = np.sqrt(-1 * results['score'])
    results = results.sort_values('RMSLE')

    return results
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [1, 10],
}

model = LGBMRegressor(random_state=random_state)
my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5)
model = LGBMRegressor(max_depth=10, n_estimators=100, random_state=random_state)
model.fit(train, y)
prediction = model.predict(test)
prediction
prediction = np.exp(prediction) - 1
prediction
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

submission_path = join(data_dir, 'sample_submission.csv')
submission = pd.read_csv(submission_path)
submission.head()
submission['price'] = prediction
submission.head()
submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, 'lgbm', '0.164399')
submission.to_csv(submission_csv_path, index=False)
print(submission_csv_path)
def save_submission(model, train, y, test, model_name, rmsle=None):
    model.fit(train, y)
    prediction = model.predict(test)
    prediction = np.expm1(prediction)
    data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"
    submission_path = join(data_dir, 'sample_submission.csv')
    submission = pd.read_csv(submission_path)
    submission['price'] = prediction
    submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, model_name, rmsle)
    submission.to_csv(submission_csv_path, index=False)
    print('{} saved!'.format(submission_csv_path))
save_submission(model, train, y, test, 'lgbm', rmsle='0.164399')
# 회고
캐글 데이터 분석 전과정은 성공적으로 진행하였습니다.

데이터에 대한 체계적인 전처리를 진행하였고, 이에 대해 결측치 처리, 이상치 제거, 피처 스케일링 등을 수행하고, 각 처리과정에 대한 설명과 코드를 작성했습니다.
또한, 선택한 회귀 모델을 학습하는 과정을 체계적으로 작성하였으며 모델 선택 이유, 학습 데이터 분할, 하이퍼파라미터 설정 등에 대한 설명이 포함되었습니다. 모델 학습 후 결과에 대한 해석을 시각화하여 쉽게 이해할 수 있도록 하였습니다.

예측 정확도가 기준 이상으로 높게 나왔고, 다양한 피처 엔지니어링과 하이퍼파라미터 튜닝 등을 통해 캐글 리더보드의 Private score 기준 110,000에 근접한 점수를 얻었습니다. 루브릭 기준 110,000 이하의 점수를 얻진 못했지만 좋은 경험이 되었습니다.
