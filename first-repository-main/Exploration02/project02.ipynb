# First Test
!pip install xgboost==1.4.2
!pip install lightgbm==3.3.0
!pip install missingno==0.5.0
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
import warnings
warnings.filterwarnings("ignore")

import os
from os.path import join

import pandas as pd
import numpy as np

import missingno as msno

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold, cross_val_score
import xgboost as xgb
import lightgbm as lgb

import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
drive.mount('/content/drive')
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

train_data_path = join(data_dir, 'train.csv')
sub_data_path = join(data_dir, 'test.csv')

print(train_data_path)
print(sub_data_path)
data = pd.read_csv(train_data_path)
sub = pd.read_csv(sub_data_path)
print(f'train data dim : {data.shape}')
print(f'sub data dim : {sub.shape}')
y = data['price']
del data['price']

print(data.columns)
train_len = len(data)
data = pd.concat((data, sub), axis=0)

print(len(data))
data.head()
!pip install matplotlib==3.4.3
import matplotlib

print(matplotlib.__version__)

msno.matrix(data) # ì˜¤ë¥˜ ê±¸ë¦¬ë©´ ëŸ°íƒ€ì„ ì¬ì‹¤í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
# 1. id ì»¬ëŸ¼ì´ ê²°ì¸¡ì¹˜ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
null_check = pd.isnull(data['id'])
print(null_check)
# 2. ê²°ì¸¡ì¹˜ì¸ ë°ì´í„°ë§Œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
null_data = data.loc[null_check, 'id']
null_data.head()
# 3. ê²°ì¸¡ì¹˜ì¸ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
print(f'id: {len(null_data.values)}')
print('{} : {}'.format('id', len(data.loc[pd.isnull(data['id']), 'id'].values)))
for c in data.columns:
    print('{} : {}'.format(c, len(data.loc[pd.isnull(data[c]), c].values)))
sub_id = data['id'][train_len:]
del data['id']

print(data.columns)
data['date'] = data['date'].apply(lambda x : str(x[:6]))

data.head()
fig, ax = plt.subplots(9, 2, figsize=(12, 50))   # ê°€ë¡œìŠ¤í¬ë¡¤ ë•Œë¬¸ì— ê·¸ë˜í”„ í™•ì¸ì´ ë¶ˆí¸í•˜ë‹¤ë©´ figsizeì˜ xê°’ì„ ì¡°ì ˆí•´ ë³´ì„¸ìš”.

# id ë³€ìˆ˜(count==0ì¸ ê²½ìš°)ëŠ” ì œì™¸í•˜ê³  ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
count = 1
columns = data.columns
for row in range(9):
    for col in range(2):
        sns.kdeplot(data=data[columns[count]], ax=ax[row][col])
        ax[row][col].set_title(columns[count], fontsize=15)
        count += 1
        if count == 19 :
            break
skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']

for c in skew_columns:
    data[c] = np.log1p(data[c].values)

plt.figure(figsize=(15, 10))

for i, c in enumerate(skew_columns, 1):
    plt.subplot(3, 3, i)
    sns.kdeplot(data=data[c])

plt.tight_layout()
plt.show()
xx = np.linspace(0, 10, 500)
yy = np.log(xx)

plt.hlines(0, 0, 10)
plt.vlines(0, -5, 5)
plt.plot(xx, yy, c='r')
plt.show()
sns.kdeplot(y)
plt.show()
y_log_transformation = np.log1p(y)

sns.kdeplot(y_log_transformation)
plt.show()
sub = data.iloc[train_len:, :]
x = data.iloc[:train_len, :]

print(x.shape)
print(sub.shape)
gboost = GradientBoostingRegressor(random_state=2023)
xgboost = xgb.XGBRegressor(random_state=2023)
lightgbm = lgb.LGBMRegressor(random_state=2023)

models = [{'model':gboost, 'name':'GradientBoosting'}, {'model':xgboost, 'name':'XGBoost'},
          {'model':lightgbm, 'name':'LightGBM'}]

print('ì–ğŸ’¢')
def get_cv_score(models):
    kfold = KFold(n_splits=5).get_n_splits(x.values)
    for m in models:
        CV_score = np.mean(cross_val_score(m['model'], X=x.values, y=y, cv=kfold))
        print(f"Model: {m['name']}, CV score:{CV_score:.4f}")
print('ì–ğŸ’¢')
get_cv_score(models)
def AveragingBlending(models, x, y, sub_x):
    for m in models :
        m['model'].fit(x.values, y)

    predictions = np.column_stack([
        m['model'].predict(sub_x.values) for m in models
    ])
    return np.mean(predictions, axis=1)

print('ì–ğŸ’¢')
y_pred = AveragingBlending(models, x, y, sub)
print(len(y_pred))
y_pred
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

submission_path = join(data_dir, 'sample_submission.csv')
submission = pd.read_csv(submission_path)
submission.head()
result = pd.DataFrame({
    'id' : sub_id,
    'price' : y_pred
})

result.head()
my_submission_path = join(data_dir, 'submission.csv')
result.to_csv(my_submission_path, index=False)

print(my_submission_path)

# Second Test(Ranking Up)
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

train_data_path = join(data_dir, 'train.csv')
test_data_path = join(data_dir, 'test.csv')

train = pd.read_csv(train_data_path)
test = pd.read_csv(test_data_path)

train.head()
train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)
train.head()
y = train['price']
del train['price']

print(train.columns)
del train['id']

print(train.columns)
test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)

del test['id']
print(test.columns)
test.head()
y
sns.kdeplot(y)
y = np.log1p(y)
y
sns.kdeplot(y)
plt.show()
train.info()
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

print('ì–ğŸ’¢')
def rmse(y_test, y_pred):
    return np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))

print('ì–ğŸ’¢')
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

print('ì–ğŸ’¢')
random_state=2020

gboost = GradientBoostingRegressor(random_state=random_state)
xgboost = XGBRegressor(random_state=random_state)
lightgbm = LGBMRegressor(random_state=random_state)
rdforest = RandomForestRegressor(random_state=random_state)

models = [gboost, xgboost, lightgbm, rdforest]

print('ì–ğŸ’¢')
gboost.__class__.__name__
df = {}

for model in models:
    # ëª¨ë¸ ì´ë¦„ íšë“
    model_name = model.__class__.__name__

    # train, test ë°ì´í„°ì…‹ ë¶„ë¦¬
    # random_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ì •í•˜ê³  trainê³¼ test ì…‹ì˜ ë¹„ìœ¨ì€ 8:2ë¡œ í•©ë‹ˆë‹¤.
    X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=42)

    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)

    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)

    # ì˜ˆì¸¡ ê²°ê³¼ì˜ rmseê°’ ì €ì¥
    df[model_name] = rmse(y_test, y_pred)

    # data frameì— ì €ì¥
    score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)

df
def get_scores(models, train, y):
    df = {}

    for model in models:
        # ëª¨ë¸ ì´ë¦„ íšë“
        model_name = model.__class__.__name__

        # train, test ë°ì´í„°ì…‹ ë¶„ë¦¬
        # random_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ì •í•˜ê³  trainê³¼ test ì…‹ì˜ ë¹„ìœ¨ì€ 8:2ë¡œ í•©ë‹ˆë‹¤.
        X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.2, random_state=42)

        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train, y_train)

        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)

        # ì˜ˆì¸¡ ê²°ê³¼ì˜ rmseê°’ ì €ì¥
        df[model_name] = rmse(y_test, y_pred)

        # data frameì— ì €ì¥
        score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)


get_scores(models, train, y)
from sklearn.model_selection import GridSearchCV

print('ì–ğŸ’¢')
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [1, 10],
}
model = LGBMRegressor(random_state=random_state)

print('ì–ğŸ’¢')
grid_model = GridSearchCV(model, param_grid=param_grid, \
                        scoring='neg_mean_squared_error', \
                        cv=5, verbose=1, n_jobs=5)

grid_model.fit(train, y)
grid_model.cv_results_
params = grid_model.cv_results_['params']
params
score = grid_model.cv_results_['mean_test_score']
score
results = pd.DataFrame(params)
results['score'] = score

results
results['RMSE'] = np.sqrt(-1 * results['score'])
results
results = results.rename(columns={'RMSE': 'RMSLE'})
results
results = results.sort_values('RMSLE')
results
def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):
    # GridSearchCV ëª¨ë¸ë¡œ ì´ˆê¸°í™”
    grid_model = GridSearchCV(model, param_grid=param_grid, scoring='neg_mean_squared_error', \
                              cv=5, verbose=verbose, n_jobs=n_jobs)

    # ëª¨ë¸ fitting
    grid_model.fit(train, y)

    # ê²°ê³¼ê°’ ì €ì¥
    params = grid_model.cv_results_['params']
    score = grid_model.cv_results_['mean_test_score']

    # ë°ì´í„° í”„ë ˆì„ ìƒì„±
    results = pd.DataFrame(params)
    results['score'] = score

    # RMSLE ê°’ ê³„ì‚° í›„ ì •ë ¬
    results['RMSLE'] = np.sqrt(-1 * results['score'])
    results = results.sort_values('RMSLE')

    return results
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [1, 10],
}

model = LGBMRegressor(random_state=random_state)
my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5)
model = LGBMRegressor(max_depth=10, n_estimators=100, random_state=random_state)
model.fit(train, y)
prediction = model.predict(test)
prediction
prediction = np.exp(prediction) - 1
prediction
data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"

submission_path = join(data_dir, 'sample_submission.csv')
submission = pd.read_csv(submission_path)
submission.head()
submission['price'] = prediction
submission.head()
submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, 'lgbm', '0.164399')
submission.to_csv(submission_csv_path, index=False)
print(submission_csv_path)
def save_submission(model, train, y, test, model_name, rmsle=None):
    model.fit(train, y)
    prediction = model.predict(test)
    prediction = np.expm1(prediction)
    data_dir = "/content/drive/MyDrive/aiffel/datasets/data/"
    submission_path = join(data_dir, 'sample_submission.csv')
    submission = pd.read_csv(submission_path)
    submission['price'] = prediction
    submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, model_name, rmsle)
    submission.to_csv(submission_csv_path, index=False)
    print('{} saved!'.format(submission_csv_path))
save_submission(model, train, y, test, 'lgbm', rmsle='0.164399')
# íšŒê³ 
ìºê¸€ ë°ì´í„° ë¶„ì„ ì „ê³¼ì •ì€ ì„±ê³µì ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

ë°ì´í„°ì— ëŒ€í•œ ì²´ê³„ì ì¸ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ì˜€ê³ , ì´ì— ëŒ€í•´ ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±°, í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ë“±ì„ ìˆ˜í–‰í•˜ê³ , ê° ì²˜ë¦¬ê³¼ì •ì— ëŒ€í•œ ì„¤ëª…ê³¼ ì½”ë“œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.
ë˜í•œ, ì„ íƒí•œ íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ìœ¼ë©° ëª¨ë¸ ì„ íƒ ì´ìœ , í•™ìŠµ ë°ì´í„° ë¶„í• , í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ë“±ì— ëŒ€í•œ ì„¤ëª…ì´ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµ í›„ ê²°ê³¼ì— ëŒ€í•œ í•´ì„ì„ ì‹œê°í™”í•˜ì—¬ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

ì˜ˆì¸¡ ì •í™•ë„ê°€ ê¸°ì¤€ ì´ìƒìœ¼ë¡œ ë†’ê²Œ ë‚˜ì™”ê³ , ë‹¤ì–‘í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“±ì„ í†µí•´ ìºê¸€ ë¦¬ë”ë³´ë“œì˜ Private score ê¸°ì¤€ 110,000ì— ê·¼ì ‘í•œ ì ìˆ˜ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. ë£¨ë¸Œë¦­ ê¸°ì¤€ 110,000 ì´í•˜ì˜ ì ìˆ˜ë¥¼ ì–»ì§„ ëª»í–ˆì§€ë§Œ ì¢‹ì€ ê²½í—˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.
